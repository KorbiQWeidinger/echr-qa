{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from src.column import Column\n",
    "from src.experiments.load_experiment import Experiment, load_experiment_df\n",
    "from src.models.claim_recall import ClaimRecall\n",
    "from src.models.manual_annotations import EvaluationBatch, Generation, SentenceWithCitations, TargetCitation\n",
    "\n",
    "used_experiments = []\n",
    "samples = []\n",
    "\n",
    "TO_BASE_MAP = {\n",
    "    Experiment.POST_HOC_LLAMA_8B: Experiment.BASE_LLAMA_8B,\n",
    "    Experiment.POST_HOC_MISTRAL_7B: Experiment.BASE_MISTRAL_7B,\n",
    "    Experiment.POST_HOC_SAUL_7B: Experiment.BASE_SAUL_7B,\n",
    "    Experiment.POST_HOC_LLAMA_70B: Experiment.BASE_LLAMA_70B,\n",
    "}\n",
    "\n",
    "def get_claim_recall(e: Experiment, row_number: int):\n",
    "    if e in TO_BASE_MAP:\n",
    "        df, _ = load_experiment_df(TO_BASE_MAP[e])\n",
    "    else: \n",
    "        df, _ = load_experiment_df(e)\n",
    "    row = df.loc[row_number]\n",
    "    claim_recall = row[Column.CLAIM_RECALL]\n",
    "    if pd.isna(claim_recall):\n",
    "        # print(f\"Claim recall not found for {e}\")\n",
    "        claim_recall = 0\n",
    "    else:\n",
    "        try: \n",
    "            claim_recall = ClaimRecall.model_validate_json(claim_recall)\n",
    "            claim_recall = claim_recall.claim_recall\n",
    "        except:\n",
    "            # print(f\"Claim recall is invalid for {e}: {claim_recall}\")\n",
    "            claim_recall = 0\n",
    "    return claim_recall\n",
    "\n",
    "def generation_from_row(row, e: Experiment):\n",
    "    if Column.GENERATED_CITATIONS in row:\n",
    "        generated_citations = row[Column.GENERATED_CITATIONS]\n",
    "        generated_citations = json.loads(generated_citations)\n",
    "        generated_citations = [SentenceWithCitations.model_validate(c) for c in generated_citations]\n",
    "    else:\n",
    "        generated_citations = None\n",
    "\n",
    "    # print(e, row[Column.GENERATED_ANSWER])\n",
    "    return Generation(\n",
    "        experiment=e.value,\n",
    "        answer=row[Column.GENERATED_ANSWER],\n",
    "        sentences_with_citations=generated_citations,\n",
    "    )\n",
    "\n",
    "def get_random_samples_for_annotation(experiments: list[Experiment]):\n",
    "    r = random.randint(0, 1100)\n",
    "    while r in used_experiments or r in {544, 161, 806, 304, 688, 178, 957, 323, 325, 336, 469, 733, 94, 872, 123, 237, 244, 378, 635}:\n",
    "        r = random.randint(0, 1100)\n",
    "\n",
    "    rows = []\n",
    "    for e in experiments:\n",
    "        df, _ = load_experiment_df(e)\n",
    "        # print(len(df), r, e)\n",
    "        row = df.iloc[r]\n",
    "        if not Column.GENERATED_CITATIONS in row or not Column.GENERATED_ANSWER in row or pd.isna(row[Column.GENERATED_ANSWER]):\n",
    "            return\n",
    "        \n",
    "        rows.append(row)\n",
    "\n",
    "    question = rows[0][Column.QUESTION]\n",
    "    target_answer = rows[0][Column.TARGET_ANSWER]\n",
    "    target_citations = rows[0][Column.TARGET_CITATIONS]\n",
    "    target_citations = json.loads(target_citations)\n",
    "    target_citations = [TargetCitation.model_validate(c) for c in target_citations]\n",
    "    \n",
    "    generations = [generation_from_row(row, e) for row, e in zip(rows, experiments)]\n",
    "\n",
    "    # have a requirement of 2 citations per answer\n",
    "    for g in generations:\n",
    "        nr_citations = 0\n",
    "        for s in g.sentences_with_citations:\n",
    "            nr_citations += len(s.citations)\n",
    "\n",
    "        if nr_citations < 2:\n",
    "            # print(\"Not enough citations\")\n",
    "            return\n",
    "            \n",
    "    # we want to have different claim recall for each\n",
    "    claim_recalls = [get_claim_recall(e, r) for e in experiments]\n",
    "    # make sure they have at least 3 different claim recalls\n",
    "    if len(set(claim_recalls)) < 2:\n",
    "        # print(f\"Not enough different claim recalls: {claim_recalls}\")\n",
    "        return\n",
    "    # print(\"Claim recalls\", claim_recalls)\n",
    "\n",
    "    evaluation_batch = EvaluationBatch(\n",
    "        axis=\"approach\",\n",
    "        question_number=r,\n",
    "        question=question,\n",
    "        answer=target_answer,\n",
    "        citations=target_citations,\n",
    "        generations=generations,\n",
    "        annotation=None,\n",
    "    )\n",
    "    samples.append(evaluation_batch.model_dump())\n",
    "    used_experiments.append(r)\n",
    "\n",
    "for i in range(1000):\n",
    "    experiments = [Experiment.POST_HOC_MISTRAL_7B, Experiment.RAG_GTR_k_10_MISTRAL_7B, Experiment.LLATRIEVAL_GTR_k_10_MISTRAL_7B, Experiment.RARR_MISTRAL_7B]\n",
    "    # experiments = [Experiment.RAG_GTR_k_10_MISTRAL_7B, Experiment.RAG_GTR_k_10_SAUL_7B, Experiment.RAG_GTR_k_10_LLAMA_8B, Experiment.RAG_GTR_k_10_LLAMA_70B]\n",
    "    get_random_samples_for_annotation(experiments)\n",
    "    # print(len(samples))\n",
    "    if len(samples) == 1:\n",
    "        break\n",
    "\n",
    "print(json.dumps(samples[0:1], indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
