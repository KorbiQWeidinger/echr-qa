{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from src.column import Column\n",
    "from src.experiments.load_experiment import Experiment, load_experiment_df\n",
    "from src.models.citation_faithfulness import CitationFaithfulness\n",
    "from src.models.claim_recall import ClaimRecall\n",
    "from src.models.bert_score import BertScore\n",
    "from src.models.manual_annotations import EvaluationBatch\n",
    "from src.models.rouge_score import RougeScore\n",
    "from src.models.citations_em import CitationsExactMatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0, 4.0, 2.0, 2.0, 5.0]\n",
      "[1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "[3.0, 3.0, 3.0, 3.0, 3.0]\n",
      "[1.5, 3.5, 3.5, 1.5, 5.0]\n",
      "{'a': 2.0, 'b': 4.0, 'c': 2.0, 'd': 2.0, 'e': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# spearmanr utils\n",
    "def to_ranks(data: list[int]):\n",
    "    current_rank = 1\n",
    "    to_rank_map = {}\n",
    "\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    while data_copy:\n",
    "        current_value = min(data_copy)\n",
    "        current_value_count = len([d for d in data_copy if d == current_value])\n",
    "\n",
    "        to_rank_map[current_value] = sum(range(current_rank, current_rank + current_value_count)) / current_value_count\n",
    "\n",
    "        current_rank += current_value_count\n",
    "        data_copy = [d for d in data_copy if d != current_value]\n",
    "\n",
    "    return [to_rank_map[d] for d in data]\n",
    "\n",
    "def values_to_ranks(data: dict[str, int]) -> dict[str, int]:\n",
    "    ranks = to_ranks(list(data.values()))\n",
    "    ranks = {k: ranks[i] for i, k in enumerate(data.keys())}\n",
    "    return ranks\n",
    "\n",
    "print(to_ranks([1,2,1,1,3]))\n",
    "print(to_ranks([1,2,3,4,5]))\n",
    "print(to_ranks([1,1,1,1,1]))\n",
    "print(to_ranks([1,2,2,1,4]))\n",
    "\n",
    "print(values_to_ranks({'a': 1, 'b': 2, 'c': 1, 'd': 1, 'e': 3}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto results retrieval methods\n",
    "\n",
    "TO_BASE_MAP = {\n",
    "    Experiment.POST_HOC_LLAMA_8B: Experiment.BASE_LLAMA_8B,\n",
    "    Experiment.POST_HOC_MISTRAL_7B: Experiment.BASE_MISTRAL_7B,\n",
    "    Experiment.POST_HOC_SAUL_7B: Experiment.BASE_SAUL_7B,\n",
    "    Experiment.POST_HOC_LLAMA_70B: Experiment.BASE_LLAMA_70B,\n",
    "}\n",
    "\n",
    "def get_claim_recall(e: Experiment, row_number: int):\n",
    "    if e in TO_BASE_MAP:\n",
    "        df, _ = load_experiment_df(TO_BASE_MAP[e])\n",
    "    else: \n",
    "        df, _ = load_experiment_df(e)\n",
    "    row = df.loc[row_number]\n",
    "    claim_recall = row[Column.CLAIM_RECALL]\n",
    "    if pd.isna(claim_recall):\n",
    "        print(f\"Claim recall not found for {e}\")\n",
    "        claim_recall = 0\n",
    "    else:\n",
    "        claim_recall = ClaimRecall.model_validate_json(claim_recall)\n",
    "        claim_recall = claim_recall.claim_recall\n",
    "    return claim_recall\n",
    "\n",
    "def get_correctness_bert(e: Experiment, row_number: int):\n",
    "    if e in TO_BASE_MAP:\n",
    "        df, _ = load_experiment_df(TO_BASE_MAP[e])\n",
    "    else: \n",
    "        df, _ = load_experiment_df(e)\n",
    "    row = df.loc[row_number]\n",
    "    bert = row[Column.CORRECTNESS_BERT]\n",
    "    if pd.isna(bert):\n",
    "        print(f\"Bert Score not found for {e}\")\n",
    "        bert = 0\n",
    "    else:\n",
    "        bert = BertScore.model_validate_json(bert)\n",
    "        bert = bert.f1\n",
    "    return bert\n",
    "\n",
    "def get_correctness_rouge(e: Experiment, row_number: int):\n",
    "    if e in TO_BASE_MAP:\n",
    "        df, _ = load_experiment_df(TO_BASE_MAP[e])\n",
    "    else: \n",
    "        df, _ = load_experiment_df(e)\n",
    "    row = df.loc[row_number]\n",
    "    rouge = row[Column.CORRECTNESS_ROUGE]\n",
    "    if pd.isna(rouge):\n",
    "        print(f\"Rouge Score not found for {e}\")\n",
    "        rouge = 0\n",
    "    else:\n",
    "        rouge = RougeScore.model_validate_json(rouge)\n",
    "        rouge = rouge.rouge_l\n",
    "    return rouge\n",
    "\n",
    "def get_citation_faithfulness_rec(e: Experiment, row_number: int):\n",
    "    df, _ = load_experiment_df(e)\n",
    "    row = df.loc[row_number]\n",
    "    if Column.CITATION_FAITHFULNESS not in row:\n",
    "        print(f\"Citation faithfulness not found for {e}\")\n",
    "        return 0\n",
    "\n",
    "    citation_faithfulness = row[Column.CITATION_FAITHFULNESS]\n",
    "    if pd.isna(citation_faithfulness):\n",
    "        print(f\"Citation faithfulness not found for {e}\")\n",
    "        citation_faithfulness = 0\n",
    "    else:\n",
    "        citation_faithfulness = CitationFaithfulness.model_validate_json(citation_faithfulness)\n",
    "        citation_faithfulness = citation_faithfulness.citation_recall\n",
    "    return citation_faithfulness\n",
    "\n",
    "def get_citation_faithfulness_prec(e: Experiment, row_number: int):\n",
    "    df, _ = load_experiment_df(e)\n",
    "    row = df.loc[row_number]\n",
    "    if Column.CITATION_FAITHFULNESS not in row:\n",
    "        print(f\"Citation faithfulness not found for {e}\")\n",
    "        return 0\n",
    "\n",
    "    citation_faithfulness = row[Column.CITATION_FAITHFULNESS]\n",
    "    if pd.isna(citation_faithfulness):\n",
    "        print(f\"Citation faithfulness not found for {e}\")\n",
    "        citation_faithfulness = 0\n",
    "    else:\n",
    "        citation_faithfulness = CitationFaithfulness.model_validate_json(citation_faithfulness)\n",
    "        citation_faithfulness = citation_faithfulness.citation_precision\n",
    "    return citation_faithfulness\n",
    "\n",
    "def get_citation_similarity(e, row_number):\n",
    "    df, _ = load_experiment_df(e)\n",
    "    row = df.loc[row_number]\n",
    "    if Column.CITATION_SIMILARITY_NLI not in row:\n",
    "        print(f\"Citation similarity not found for {e}\")\n",
    "        return 0\n",
    "\n",
    "    citation_similarity = row[Column.CITATION_SIMILARITY_NLI]\n",
    "    if pd.isna(citation_similarity):\n",
    "        print(f\"Citation similarity not found for {e}\")\n",
    "        citation_similarity = 0\n",
    "    return citation_similarity \n",
    "\n",
    "def get_citation_similarity_em(e, row_number):\n",
    "    df, _ = load_experiment_df(e)\n",
    "    row = df.loc[row_number]\n",
    "    if Column.CITATION_SIMILARITY_EM not in row:\n",
    "        print(f\"Citation similarity not found for {e}\")\n",
    "        return 0\n",
    "\n",
    "    citation_similarity = row[Column.CITATION_SIMILARITY_EM]\n",
    "    if pd.isna(citation_similarity):\n",
    "        print(f\"Citation similarity not found for {e}\")\n",
    "        citation_similarity = 0\n",
    "    else: \n",
    "        citation_similarity = CitationsExactMatch.model_validate_json(citation_similarity)\n",
    "        citation_similarity = citation_similarity.f1\n",
    "    return citation_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "def get_average_ranks(ranks: list[dict[str, int]]):\n",
    "    ranks = pd.DataFrame(ranks)\n",
    "    return ranks.mean()\n",
    "\n",
    "\n",
    "def get_annotation_data(file_name: str):\n",
    "    with open(file_name) as f:\n",
    "        data = json.load(f)\n",
    "        data = [EvaluationBatch.model_validate(d) for d in data if d['annotation'] != None]\n",
    "        data = [d for d in data if d.annotation != None]\n",
    "        return data  \n",
    "\n",
    "# data = get_annotation_data('data/expert_annotations_rag.json')    \n",
    "data = get_annotation_data('data/expert_annotations_mistral.json')\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expert_correctness_order(e: EvaluationBatch):\n",
    "    experiment_order = [g.experiment for g in e.generations]\n",
    "    annotation_order = e.annotation.claim_order\n",
    "    annotation_order_map = {experiment_order[i]: int(annotation_order[i]) for i in range(4)}\n",
    "    return annotation_order_map\n",
    "\n",
    "correctness_avg_ranks = get_average_ranks([values_to_ranks(get_expert_correctness_order(e)) for e in data])\n",
    "\n",
    "def get_expert_groundedness_order(e: EvaluationBatch):\n",
    "    experiment_order = [g.experiment for g in e.generations]\n",
    "    annotation_order = e.annotation.citation_faithfulness_order\n",
    "    annotation_order_map = {experiment_order[i]: int(annotation_order[i]) for i in range(4)}\n",
    "    return annotation_order_map\n",
    "\n",
    "groundedness_avg_ranks = get_average_ranks([values_to_ranks(get_expert_groundedness_order(e)) for e in data])\n",
    "\n",
    "def get_expert_citation_relevance_order(e: EvaluationBatch):\n",
    "    experiment_order = [g.experiment for g in e.generations]\n",
    "    annotation_order = e.annotation.citation_similarity_order\n",
    "    annotation_order_map = {experiment_order[i]: int(annotation_order[i]) for i in range(4)}\n",
    "    return annotation_order_map\n",
    "\n",
    "citation_relevance_avg_ranks = get_average_ranks([values_to_ranks(get_expert_citation_relevance_order(e)) for e in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_automatic_correctness_ranks(e: EvaluationBatch):\n",
    "    row_number = e.question_number\n",
    "    experiment_order = [g.experiment for g in e.generations]\n",
    "    claim_recalls = [get_claim_recall(e, row_number) for e in experiment_order]\n",
    "    # we negate numbers because values_to_ranks assumes lower is better\n",
    "    claim_recall_map = {experiment_order[i]: -claim_recalls[i] for i in range(4)}\n",
    "    claim_recall_map = values_to_ranks(claim_recall_map)\n",
    "    return claim_recall_map\n",
    "\n",
    "avg_claim_recall_ranks = get_average_ranks([get_automatic_correctness_ranks(e) for e in data])\n",
    "\n",
    "def get_automatic_groundedness_prec_ranks(e: EvaluationBatch):\n",
    "    # Note: in code prec and rec are switched\n",
    "    row_number = e.question_number\n",
    "    experiment_order = [g.experiment for g in e.generations]\n",
    "    citation_faithfulness = [get_citation_faithfulness_rec(e, row_number) for e in experiment_order]\n",
    "    citation_faithfulness_map = {experiment_order[i]: -citation_faithfulness[i] for i in range(4)}\n",
    "    citation_faithfulness_map = values_to_ranks(citation_faithfulness_map)\n",
    "    return citation_faithfulness_map\n",
    "\n",
    "def get_automatic_groundedness_rec_ranks(e: EvaluationBatch):\n",
    "    # Note: in code prec and rec are switched\n",
    "    row_number = e.question_number\n",
    "    experiment_order = [g.experiment for g in e.generations]\n",
    "    citation_faithfulness = [get_citation_faithfulness_prec(e, row_number) for e in experiment_order]\n",
    "    citation_faithfulness_map = {experiment_order[i]: -citation_faithfulness[i] for i in range(4)}\n",
    "    citation_faithfulness_map = values_to_ranks(citation_faithfulness_map)\n",
    "    return citation_faithfulness_map\n",
    "\n",
    "avg_citation_faithfulness_ranks = get_average_ranks([get_automatic_groundedness_rec_ranks(e) for e in data])\n",
    "\n",
    "def get_automatic_citation_relevance_ranks(e: EvaluationBatch):\n",
    "    row_number = e.question_number\n",
    "    experiment_order = [g.experiment for g in e.generations]\n",
    "    citation_similarity = [get_citation_similarity(e, row_number) for e in experiment_order]\n",
    "    citation_similarity_map = {experiment_order[i]: -citation_similarity[i] for i in range(4)}\n",
    "    citation_similarity_map = values_to_ranks(citation_similarity_map)\n",
    "    return citation_similarity_map\n",
    "\n",
    "avg_automatic_citation_relevance_ranks = get_average_ranks([get_automatic_citation_relevance_ranks(e) for e in data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG & 2.45 & 2.10 & 2.15 & 1.70 & 2.15 & 2.67 \\\\\n",
      "Llatrieval & 1.80 & 2.00 & 2.10 & 1.43 & 2.15 & 2.20 \\\\\n",
      "Post-hoc & 2.90 & 2.95 & 3.30 & 3.90 & 3.35 & 3.15 \\\\\n",
      "RARR & 2.85 & 2.95 & 2.45 & 2.98 & 2.35 & 1.98 \\\\\n"
     ]
    }
   ],
   "source": [
    "# Print my nice latex table \n",
    "mapping1 = {\n",
    "    Experiment.RAG_GTR_k_10_MISTRAL_7B.value: \"RAG\",\n",
    "    Experiment.LLATRIEVAL_GTR_k_10_MISTRAL_7B.value: \"Llatrieval\",\n",
    "    Experiment.POST_HOC_MISTRAL_7B.value: \"Post-hoc\",\n",
    "    Experiment.RARR_MISTRAL_7B.value: \"RARR\"\n",
    "}\n",
    "\n",
    "mapping2 = {\n",
    "    Experiment.RAG_GTR_k_10_MISTRAL_7B.value: \"Mistral-7B\",\n",
    "    Experiment.RAG_GTR_k_10_SAUL_7B.value: \"SaulLM-7B\",\n",
    "    Experiment.RAG_GTR_k_10_LLAMA_8B.value: \"Llama-3-8B\",\n",
    "    Experiment.RAG_GTR_k_10_LLAMA_70B.value: \"Llama-3-70B\",\n",
    "}\n",
    "\n",
    "for k in mapping1.keys():\n",
    "    print(f\"{mapping1[k]} & {correctness_avg_ranks[k]:.2f} & {avg_claim_recall_ranks[k]:.2f} & {groundedness_avg_ranks[k]:.2f} & {avg_citation_faithfulness_ranks[k]:.2f} & {citation_relevance_avg_ranks[k]:.2f} & {avg_automatic_citation_relevance_ranks[k]:.2f} \\\\\\\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim Recall vs Expert Correctness: 0.09310996649714702, 0.24157076977032546\n"
     ]
    }
   ],
   "source": [
    "# lets get the correlation between expert correctness and claim recall\n",
    "\n",
    "def sort_by_keys(d: dict[str, int]) -> dict[str, int]:\n",
    "    return {k: d[k] for k in sorted(d.keys())}\n",
    "\n",
    "def transform_to_flat_ranks(func: callable):\n",
    "    r = [values_to_ranks(func(e)) for e in data]\n",
    "    r = [sort_by_keys(r) for r in r]\n",
    "    r = [list(r.values()) for r in r]\n",
    "    r = to_ranks([v for r in r for v in r])\n",
    "    return r\n",
    "\n",
    "expert_correctness_ranks = transform_to_flat_ranks(get_expert_correctness_order)\n",
    "auto_claim_recall_ranks = transform_to_flat_ranks(get_automatic_correctness_ranks)\n",
    "cr_corr, cr_p = stats.spearmanr(auto_claim_recall_ranks, expert_correctness_ranks)\n",
    "print(f\"Claim Recall vs Expert Correctness: {cr_corr}, {cr_p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert vs Expert Correctness: 0.23450358479614078, 0.0028382086911872915\n"
     ]
    }
   ],
   "source": [
    "# lets get the correlation between expert correctness and bert score\n",
    "\n",
    "def get_automatic_bert_ranks(e: EvaluationBatch):\n",
    "    row_number = e.question_number\n",
    "    experiment_order = [g.experiment for g in e.generations]\n",
    "    bert_scores = [get_correctness_bert(e, row_number) for e in experiment_order]\n",
    "    # we negate numbers because values_to_ranks assumes lower is better\n",
    "    bert_map = {experiment_order[i]: -bert_scores[i] for i in range(4)}\n",
    "    bert_map = values_to_ranks(bert_map)\n",
    "    return bert_map\n",
    "\n",
    "\n",
    "auto_bert_ranks = transform_to_flat_ranks(get_automatic_bert_ranks)\n",
    "\n",
    "bert_corr, bert_p = stats.spearmanr(auto_bert_ranks, expert_correctness_ranks)\n",
    "print(f\"Bert vs Expert Correctness: {bert_corr}, {bert_p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge vs Expert Correctness: 0.09458406019460154, 0.23416719435925173\n"
     ]
    }
   ],
   "source": [
    "# rouge l score vs expert correctness\n",
    "\n",
    "def get_automatic_rouge_ranks(e: EvaluationBatch):\n",
    "    row_number = e.question_number\n",
    "    experiment_order = [g.experiment for g in e.generations]\n",
    "    rouge_scores = [get_correctness_rouge(e, row_number) for e in experiment_order]\n",
    "    # we negate numbers because values_to_ranks assumes lower is better\n",
    "    rouge_map = {experiment_order[i]: -rouge_scores[i] for i in range(4)}\n",
    "    rouge_map = values_to_ranks(rouge_map)\n",
    "    return rouge_map\n",
    "\n",
    "auto_rouge_ranks = transform_to_flat_ranks(get_automatic_rouge_ranks)\n",
    "\n",
    "rouge_corr, rouge_p = stats.spearmanr(auto_rouge_ranks, expert_correctness_ranks)\n",
    "print(f\"Rouge vs Expert Correctness: {rouge_corr}, {rouge_p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citation Faithfulness Rec vs Expert Groundedness: 0.3195694922029133, 3.80330930979127e-05\n"
     ]
    }
   ],
   "source": [
    "# Correlation between expert groundedness and citation faithfulness\n",
    "\n",
    "expert_groundedness_ranks = transform_to_flat_ranks(get_expert_groundedness_order)\n",
    "\n",
    "auto_citation_faithfulness_ranks = transform_to_flat_ranks(get_automatic_groundedness_ranks)\n",
    "\n",
    "cf_corr, cf_p = stats.spearmanr(auto_citation_faithfulness_ranks, expert_groundedness_ranks)\n",
    "print(f\"Citation Faithfulness Rec vs Expert Groundedness: {cf_corr}, {cf_p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citation Faithfulness Prec vs Expert Groundedness: 0.2628380116576318, 0.0007855483491362403\n"
     ]
    }
   ],
   "source": [
    "auto_citation_faithfulness_ranks = transform_to_flat_ranks(get_automatic_groundedness_prec_ranks)\n",
    "\n",
    "cfp_corr, cfp_p = stats.spearmanr(auto_citation_faithfulness_ranks, expert_groundedness_ranks)\n",
    "print(f\"Citation Faithfulness Prec vs Expert Groundedness: {cfp_corr}, {cfp_p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citation Similarity vs Expert Citation Relevance: 0.3839582852951068, 5.383449629975013e-07\n"
     ]
    }
   ],
   "source": [
    "# Correlation between expert citation relevance and citation similarity\n",
    "\n",
    "expert_citation_relevance_ranks = transform_to_flat_ranks(get_expert_citation_relevance_order)\n",
    "\n",
    "auto_citation_similarity_ranks = transform_to_flat_ranks(get_automatic_citation_relevance_ranks)\n",
    "\n",
    "cs_corr, cs_p = stats.spearmanr(auto_citation_similarity_ranks, expert_citation_relevance_ranks)\n",
    "print(f\"Citation Similarity vs Expert Citation Relevance: {cs_corr}, {cs_p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citation Similarity EM vs Expert Citation Relevance: 0.10353380421399883, 0.192627199609641\n"
     ]
    }
   ],
   "source": [
    "# Correlation between expert citation relevance and citation similarity em\n",
    "\n",
    "def get_automatic_citation_relevance_em_ranks(e: EvaluationBatch):\n",
    "    row_number = e.question_number\n",
    "    experiment_order = [g.experiment for g in e.generations]\n",
    "    citation_similarity = [get_citation_similarity_em(e, row_number) for e in experiment_order]\n",
    "    citation_similarity_map = {experiment_order[i]: -citation_similarity[i] for i in range(4)}\n",
    "    citation_similarity_map = values_to_ranks(citation_similarity_map)\n",
    "    return citation_similarity_map\n",
    "\n",
    "auto_citation_similarity_em_ranks = transform_to_flat_ranks(get_automatic_citation_relevance_em_ranks)\n",
    "\n",
    "cs_em_corr, cs_em_p = stats.spearmanr(auto_citation_similarity_em_ranks, expert_citation_relevance_ranks)\n",
    "print(f\"Citation Similarity EM vs Expert Citation Relevance: {cs_em_corr}, {cs_em_p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-L & 0.09 & 0.23416719435925173 \\\\\n",
      "BERTScore & 0.23 & 0.0028382086911872915 \\\\\n",
      "Claim Recall & 0.09 & 0.24157076977032546 \\\\\n",
      "Groundedness (Rec) & 0.26 & 0.0007855483491362403 \\\\\n",
      "Groundedness (Prec) & 0.32 & 3.80330930979127e-05 \\\\\n",
      "Citation EM & 0.10 & 0.192627199609641 \\\\\n",
      "Citation Recall & 0.38 & 5.383449629975013e-07 \\\\\n"
     ]
    }
   ],
   "source": [
    "# print as latex table\n",
    "\n",
    "print(f\"ROUGE-L & {rouge_corr:.2f} & {rouge_p} \\\\\\\\\")\n",
    "print(f\"BERTScore & {bert_corr:.2f} & {bert_p} \\\\\\\\\")\n",
    "print(f\"Claim Recall & {cr_corr:.2f} & {cr_p} \\\\\\\\\")\n",
    "# Note that in code Prec and Rec are swapped compared to the paper!!!\n",
    "print(f\"Groundedness (Rec) & {cfp_corr:.2f} & {cfp_p} \\\\\\\\\")\n",
    "print(f\"Groundedness (Prec) & {cf_corr:.2f} & {cf_p} \\\\\\\\\")\n",
    "print(f\"Citation EM & {cs_em_corr:.2f} & {cs_em_p} \\\\\\\\\")\n",
    "print(f\"Citation Recall & {cs_corr:.2f} & {cs_p} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total & 18.75\\% \\\\\n",
      "rag_gtr_k_10_llama_8b & 20.00\\% \\\\ 4 20\n",
      "rag_gtr_k_10_mistral_7b & 15.00\\% \\\\ 6 40\n",
      "llatrieval_gtr_k_10_mistral_7b & 25.00\\% \\\\ 5 20\n",
      "rarr_mistral_7b & 15.00\\% \\\\ 3 20\n",
      "post_hoc_mistral_7b & 15.00\\% \\\\ 3 20\n",
      "rag_gtr_k_10_saul_7b & 30.00\\% \\\\ 6 20\n",
      "rag_gtr_k_10_llama_70b & 15.00\\% \\\\ 3 20\n"
     ]
    }
   ],
   "source": [
    "# Better than target\n",
    "\n",
    "def get_better_than_target(e: EvaluationBatch):\n",
    "    experiment_order = [g.experiment for g in e.generations]\n",
    "    mapping = {\n",
    "        experiment_order[0]: \"1\",\n",
    "        experiment_order[1]: \"2\",\n",
    "        experiment_order[2]: \"3\",\n",
    "        experiment_order[3]: \"4\"\n",
    "    }\n",
    "\n",
    "    better_than_target = e.annotation.better_than_target\n",
    "    better_than_target_map = {e: True if mapping[e] in better_than_target else False for e in experiment_order}\n",
    "    return better_than_target_map\n",
    "\n",
    "better_than_target = [get_better_than_target(d) for d in data]\n",
    "\n",
    "# How many times do we get better than target\n",
    "better_total = sum([sum([1 for v in d.values() if v]) for d in better_than_target]) \n",
    "total = sum([len(d) for d in better_than_target])\n",
    "print(f\"Total & {(better_total / total * 100):.2f}\\\\% \\\\\\\\\")\n",
    "\n",
    "experiments = {e for b in better_than_target for e in b.keys()}\n",
    "\n",
    "for e in experiments:\n",
    "    better_total = sum([1 for d in better_than_target if e in d and d[e]])\n",
    "    total = len([1 for d in better_than_target if e in d])\n",
    "    print(f\"{e} & {better_total / total * 100:.2f}\\\\% \\\\\\\\\", better_total, total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
